{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users/junzheyin/iris/src', '/users/junzheyin/iris/src', '/users/junzheyin/iris/src/models', '/users/junzheyin/anaconda3/envs/iris/lib/python38.zip', '/users/junzheyin/anaconda3/envs/iris/lib/python3.8', '/users/junzheyin/anaconda3/envs/iris/lib/python3.8/lib-dynload', '', '/users/junzheyin/.local/lib/python3.8/site-packages', '/users/junzheyin/anaconda3/envs/iris/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/users/junzheyin/iris/src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import Batch\n",
    "from models.kv_caching import KeysValues\n",
    "from models.slicer import Embedder, Head\n",
    "from models.tokenizer import Tokenizer\n",
    "from models.transformer import Transformer, TransformerConfig\n",
    "from utils import init_weights, LossWithIntermediateLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes=time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes=time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 6, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/40\n",
    "            #image = 2*image-1 #normalize to [-1,1]\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32183 3493 3560\n"
     ]
    }
   ],
   "source": [
    "# develop dataset\n",
    "from torch.cuda.amp import autocast\n",
    "#from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "\n",
    "df_train = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14_20.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_s = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train_s[0].to_list()\n",
    "dataset_train_del = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_test = pd.read_csv('/users/hbi/taming-transformers/testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('/users/hbi/taming-transformers/validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('/users/hbi/taming-transformers/training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_dw = pd.read_csv('/users/hbi/taming-transformers/training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_re = pd.read_csv('/users/hbi/taming-transformers/training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "data_list = [dataset_train_aa, dataset_train_dw, dataset_train_del, dataset_train_re]\n",
    "train_aadedwre = torch.utils.data.ConcatDataset(data_list)\n",
    "\n",
    "print(len(dataset_train), len(dataset_test), len(dataset_vali))\n",
    "loaders = { 'train' :DataLoader(train_aadedwre, batch_size=1, shuffle=True, num_workers=8),\n",
    "            'test' :DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size=1, shuffle=False, num_workers=8),\n",
    "          \n",
    "          'train_aa5' :DataLoader(dataset_train_aa, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_dw5' :DataLoader(dataset_train_dw, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_del5' :DataLoader(dataset_train_del, batch_size=1, shuffle=True, num_workers=8),\n",
    "          'train_re5' :DataLoader(dataset_train_re, batch_size=1, shuffle=False, num_workers=8),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'defaults': ['_self_', {'tokenizer': 'default'}, {'world_model': 'default'}, {'actor_critic': 'default'}, {'env': 'default'}, {'datasets': 'default'}], 'wandb': {'mode': 'disabled', 'project': 'iris', 'entity': None, 'name': None, 'group': None, 'tags': None, 'notes': None}, 'initialization': {'path_to_checkpoint': None, 'load_tokenizer': False, 'load_world_model': False, 'load_actor_critic': False}, 'common': {'epochs': 600, 'device': 'cuda:0', 'do_checkpoint': True, 'seed': 0, 'sequence_length': '${world_model.max_blocks}', 'resume': False}, 'collection': {'train': {'num_envs': 1, 'stop_after_epochs': 500, 'num_episodes_to_save': 10, 'config': {'epsilon': 0.01, 'should_sample': True, 'temperature': 1.0, 'num_steps': 200, 'burn_in': '${training.actor_critic.burn_in}'}}, 'test': {'num_envs': 8, 'num_episodes_to_save': '${collection.train.num_episodes_to_save}', 'config': {'epsilon': 0.0, 'should_sample': True, 'temperature': 0.5, 'num_episodes': 16, 'burn_in': '${training.actor_critic.burn_in}'}}}, 'training': {'should': True, 'learning_rate': 0.0001, 'sampling_weights': [0.125, 0.125, 0.25, 0.5], 'tokenizer': {'batch_num_samples': 256, 'grad_acc_steps': 1, 'max_grad_norm': 10.0, 'start_after_epochs': 5, 'steps_per_epoch': 200}, 'world_model': {'batch_num_samples': 64, 'grad_acc_steps': 1, 'max_grad_norm': 10.0, 'weight_decay': 0.01, 'start_after_epochs': 25, 'steps_per_epoch': 200}, 'actor_critic': {'batch_num_samples': 64, 'grad_acc_steps': 1, 'max_grad_norm': 10.0, 'start_after_epochs': 50, 'steps_per_epoch': 200, 'imagine_horizon': '${common.sequence_length}', 'burn_in': 20, 'gamma': 0.995, 'lambda_': 0.95, 'entropy_weight': 0.001}}, 'evaluation': {'should': False, 'every': 5, 'tokenizer': {'batch_num_samples': '${training.tokenizer.batch_num_samples}', 'start_after_epochs': '${training.tokenizer.start_after_epochs}', 'save_reconstructions': True}, 'world_model': {'batch_num_samples': '${training.world_model.batch_num_samples}', 'start_after_epochs': '${training.world_model.start_after_epochs}'}, 'actor_critic': {'num_episodes_to_save': '${training.actor_critic.batch_num_samples}', 'horizon': '${training.actor_critic.imagine_horizon}', 'start_after_epochs': '${training.actor_critic.start_after_epochs}'}}}\n",
      "<function set_seed at 0x7f5e0b5f73a0>\n"
     ]
    }
   ],
   "source": [
    "from utils import configure_optimizer, EpisodeDirManager, set_seed\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "config =OmegaConf.load('/users/junzheyin/iris/config/trainer.yaml')\n",
    "cfg=config\n",
    "# Access the configuration and perform further operations\n",
    "# For example, print the contents of the configuration\n",
    "print(config)\n",
    "\n",
    "\n",
    "if config.common.seed is not None:\n",
    "        set_seed(config.common.seed)\n",
    "\n",
    "print(set_seed)\n",
    "\n",
    "\n",
    "@hydra.main(config_path='/users/junzheyin/iris/config/', config_name='trainer.yaml')\n",
    "def config_function(cfg):\n",
    "    # Access the configuration\n",
    "    config_file = OmegaConf.load(cfg)\n",
    "  \n",
    "    return config_file\n",
    "\n",
    "\n",
    "cfg_worldmodel = config_function('/users/junzheyin/iris/config/world_model/default.yaml')\n",
    "cfg_tokenizer = config_function('/users/junzheyin/iris/config/tokenizer/default.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(cfg.common.device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from agent import Agent\n",
    "from collector import Collector\n",
    "from make_reconstructions import make_reconstructions_from_batch\n",
    "\n",
    "from utils import configure_optimizer, EpisodeDirManager, set_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=1024, bias=True)\n",
      "Tokenizer : shape of latent is (256, 16, 16).\n"
     ]
    }
   ],
   "source": [
    "from models.transformer import Transformer, TransformerConfig\n",
    "\n",
    "block_mask = torch.cat([torch.ones(768), torch.zeros(1536)])\n",
    "\n",
    "config = TransformerConfig(tokens_per_block=768, max_blocks=3, attention=\"causal\", num_layers=6, num_heads=8, embed_dim=256, embed_pdrop=0.1, resid_pdrop=0.1, attn_pdrop=0.1)\n",
    "\n",
    "transformer = Transformer(config)\n",
    "head_observations = nn.Linear(256, 1024)\n",
    "print(head_observations)\n",
    "tokenizer = instantiate(cfg_tokenizer)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer : shape of latent is (256, 16, 16).\n"
     ]
    }
   ],
   "source": [
    "from utils import configure_optimizer, EpisodeDirManager, set_seed\n",
    "from models.world_model import WorldModel\n",
    "tokenizer = instantiate(cfg_tokenizer)\n",
    "\n",
    "world_model = WorldModel(obs_vocab_size=tokenizer.vocab_size,config=instantiate(cfg_worldmodel))\n",
    "world_model.to(device)\n",
    "\n",
    "optimizer_trans = configure_optimizer(world_model, cfg.training.learning_rate, cfg.training.world_model.weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_opt = torch.load('/space/junzheyin/vqvae_checkpoint_epoch58', map_location=device)\n",
    "\n",
    "# Load model's state dict\n",
    "model_state_dict = ckpt_opt['model_state_dict']\n",
    "tokenizer.load_state_dict(model_state_dict)\n",
    "tokenizer.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (576) must match the size of tensor b (768) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m loaders[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     input_image\u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device) \n\u001b[0;32m---> 14\u001b[0m     losses \u001b[39m=\u001b[39m world_model\u001b[39m.\u001b[39;49mcompute_loss(input_image,tokenizer)\n\u001b[1;32m     15\u001b[0m     loss_total_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mloss_total \n\u001b[1;32m     16\u001b[0m     loss_total_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_total_step\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/iris/src/models/world_model.py:97\u001b[0m, in \u001b[0;36mWorldModel.compute_loss\u001b[0;34m(self, batch, tokenizer, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens\u001b[39m.\u001b[39mview(\u001b[39m4\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[39m# print(self.obs_tokens)\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     x, logits_observations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_tokens)\n\u001b[1;32m     99\u001b[0m     target_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombined_predicted_image\n\u001b[1;32m    100\u001b[0m     \u001b[39m#print(\"Target observation\", target_obs.size())\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39m# Compute the loss between the predicted observations and the real target observations\u001b[39;00m\n",
      "File \u001b[0;32m~/iris/src/models/world_model.py:65\u001b[0m, in \u001b[0;36mWorldModel.forward\u001b[0;34m(self, obs_tokens)\u001b[0m\n\u001b[1;32m     63\u001b[0m context_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_embed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens[:, :\u001b[39m768\u001b[39m])\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m predicted_image\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_embed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens[:, \u001b[39m768\u001b[39m:])\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m combined_context_image \u001b[39m=\u001b[39m context_image \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPositional\n\u001b[1;32m     66\u001b[0m combined_context_image\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombined_predicted_image \u001b[39m=\u001b[39m predicted_image \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPredicted\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (576) must match the size of tensor b (768) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    save_epoch = epoch in [0,2,4,9,19,29,39,49,59,79]\n",
    "    #save_epoch = epoch in [2]\n",
    "    loss_total_epoch=0.0\n",
    "    optimizer_trans.zero_grad()\n",
    "    intermediate_losses = {}\n",
    "    print(\"epoch {}\".format(epoch)) \n",
    "    loss_total_step=0\n",
    "    \n",
    "    i=0\n",
    "    for images in loaders['train']:\n",
    "        input_image= images.to(device) \n",
    "\n",
    "        losses = world_model.compute_loss(input_image,tokenizer)\n",
    "        loss_total_step += losses.loss_total \n",
    "        loss_total_epoch += loss_total_step.item()\n",
    "        if (i+1) % 16 == 0:\n",
    "            (loss_total_step/16).backward()\n",
    "            optimizer_trans.step()\n",
    "            optimizer_trans.zero_grad()\n",
    "            print(\"Losses: Total = {:.4f}\".format(loss_total_step.item()))\n",
    "        \n",
    "        for loss_name, loss_value in losses.intermediate_losses.items():\n",
    "            intermediate_losses[f\"{str(world_model)}/train/{loss_name}\"] = loss_value/16\n",
    "    \n",
    "        \n",
    "    metrics = {f'{str(world_model)}/train/total_loss': loss_total_epoch, **intermediate_losses}\n",
    "    print(\"Epoch {}: Total Loss = {:.4f}\".format(epoch, metrics[f'{str(world_model)}/train/total_loss']))\n",
    "\n",
    "    if epoch in save_epoch:\n",
    "        torch.save({\n",
    "        'model_state_dict': world_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_trans.state_dict(),\n",
    "        }, '/space/zboucher/iris_1/src/checkpoint/transformer_{}'.format(epoch+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trans_Trainer:\n",
    "    def __init__(self, cfg_tokenizer, trainloader, device):\n",
    "        \n",
    "        self.trainloader= trainloader\n",
    "        self.device = device\n",
    "        self.tokenizer = instantiate(cfg_tokenizer)\n",
    "        self.world_model = WorldModel(obs_vocab_size=self.tokenizer.vocab_size, config=instantiate(cfg_worldmodel))\n",
    "        self.agent = Agent(self.tokenizer, self.world_model).to(self.device)\n",
    "        self.epochs_to_save = [84, 89, 94, 99, 104, 109]\n",
    "        self.all_metrics = []\n",
    "        print(f'{sum(p.numel() for p in self.agent.tokenizer.parameters())} parameters in agent.tokenizer')\n",
    "        print(f'{sum(p.numel() for p in self.agent.world_model.parameters())} parameters in agent.world_model')\n",
    "\n",
    "        self.optimizer_tokenizer = torch.optim.Adam(self.agent.tokenizer.parameters(), lr=0.0001)\n",
    "        self.optimizer_world_model = configure_optimizer(self.agent.world_model, 0.0001, 0.01)\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        start_epoch=80\n",
    "        end_epoch=110\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            #print(f\"\\nEpoch {epoch} / {self.cfg.common.epochs}\\n\")\n",
    "            print(f\"\\nEpoch {epoch} / {80}\\n\")\n",
    "            start_time = time.time()\n",
    "            to_log = []\n",
    "            to_log += self.train_agent(epoch)\n",
    "            to_log.append({'duration': (time.time() - start_time) / 3600})\n",
    "       \n",
    "\n",
    "    def train_agent(self, epoch: int) -> None:\n",
    "        self.agent.train()\n",
    "        self.agent.zero_grad()\n",
    "\n",
    "        metrics_worldmodel= {}\n",
    "\n",
    "        #cfg_tokenizer = self.cfg.training.tokenizer\n",
    "        #w = self.cfg.training.sampling_weights\n",
    "        #if epoch > cfg_tokenizer.start_after_epochs:\n",
    "\n",
    "        if epoch >= 0:\n",
    "            \n",
    "            print(\"Start Tokenizer training\")\n",
    "            metrics_worldmodel = self.train_component(self.agent.world_model, self.optimizer_world_model, self.trainloader, epoch)\n",
    "\n",
    "        return [{'epoch': epoch, **metrics_tokenizer}]\n",
    "\n",
    "    def train_component(self, component: nn.Module, optimizer: torch.optim.Optimizer, trainloader: DataLoader,epoch: int):\n",
    "        loss_total_epoch = 0.0\n",
    "        loss_total_epoch = 0.0\n",
    "        intermediate_losses = defaultdict(float)\n",
    "        optimizer.zero_grad()\n",
    "        i=0\n",
    "        for images in trainloader:\n",
    "            input_image= images.to(device) \n",
    "            print(device)\n",
    "\n",
    "            losses = component.compute_loss(input_image,tokenizer)\n",
    "            loss_total_step += losses.loss_total \n",
    "            loss_total_epoch += loss_total_step.item()\n",
    "            if (i+1) % 16 == 0:\n",
    "                (loss_total_step/16).backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                print(\"Losses: Total = {:.4f}\".format(loss_total_step.item()))\n",
    "            \n",
    "            for loss_name, loss_value in losses.intermediate_losses.items():\n",
    "                intermediate_losses[f\"{str(component)}/train/{loss_name}\"] = loss_value/64\n",
    "        \n",
    "            \n",
    "        metrics = {f'{str(component)}/train/total_loss': loss_total_epoch, **intermediate_losses}\n",
    "        print(\"Epoch {}: Total Loss = {:.4f}\".format(epoch, metrics[f'{str(component)}/train/total_loss']))\n",
    "\n",
    "        if epoch in self.epochs_to_save:\n",
    "            torch.save({\n",
    "            'model_state_dict': component.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, '/space/zboucher/iris_1/src/checkpoint/transformer_{}'.format(epoch+1))\n",
    "\n",
    "\n",
    "        metrics = {f'{str(component)}/train/total_loss': loss_total_epoch, **intermediate_losses}\n",
    "        self.all_metrics.append(metrics)  # Save metrics for the current epoch to the list\n",
    "        np.save('/space/zboucher/iris_1/src/checkpoint/all_metric.npy', self.all_metrics)\n",
    "        return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer : shape of latent is (256, 16, 16).\n",
      "torch.Size([2304])\n",
      "positional torch.Size([768, 256])\n",
      "torch.Size([4, 768, 256])\n",
      "positional torch.Size([4, 1536, 256])\n",
      "86853249 parameters in agent.tokenizer\n",
      "5264384 parameters in agent.world_model\n",
      "\n",
      "Epoch 80 / 80\n",
      "\n",
      "Start Tokenizer training\n",
      "cuda:1\n",
      "torch.Size([4, 9, 1, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset\u001b[39m=\u001b[39mloaders[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m trainer \u001b[39m=\u001b[39m Trans_Trainer(cfg_tokenizer,train_dataset, device)\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mTrans_Trainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     24\u001b[0m to_log \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m to_log \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_agent(epoch)\n\u001b[1;32m     26\u001b[0m to_log\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m: (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time) \u001b[39m/\u001b[39m \u001b[39m3600\u001b[39m})\n",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m, in \u001b[0;36mTrans_Trainer.train_agent\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart Tokenizer training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     metrics_worldmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_component(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mworld_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_world_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainloader, epoch)\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m [{\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmetrics_tokenizer}]\n",
      "Cell \u001b[0;32mIn[20], line 56\u001b[0m, in \u001b[0;36mTrans_Trainer.train_component\u001b[0;34m(self, component, optimizer, trainloader, epoch)\u001b[0m\n\u001b[1;32m     53\u001b[0m input_image\u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device) \n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(device)\n\u001b[0;32m---> 56\u001b[0m losses \u001b[39m=\u001b[39m component\u001b[39m.\u001b[39;49mcompute_loss(input_image,tokenizer)\n\u001b[1;32m     57\u001b[0m loss_total_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mloss_total \n\u001b[1;32m     58\u001b[0m loss_total_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_total_step\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/world/World_Model/models/world_model.py:89\u001b[0m, in \u001b[0;36mWorldModel.compute_loss\u001b[0;34m(self, batch, tokenizer, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m shape \u001b[39m=\u001b[39m batch_obs\u001b[39m.\u001b[39mshape\n\u001b[1;32m     88\u001b[0m \u001b[39mprint\u001b[39m(shape)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens\u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(batch_obs[:,:,:,:,:], should_preprocess\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mtokens \n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens\u001b[39m.\u001b[39mview(\u001b[39m4\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_tokens)\n",
      "File \u001b[0;32m~/world/World_Model/models/tokenizer/tokenizer.py:73\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[0;34m(self, x, should_preprocess)\u001b[0m\n\u001b[1;32m     69\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:])\n\u001b[1;32m     70\u001b[0m \u001b[39m#print(\"input x shape\", x.shape)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m#x=x.squeeze(1)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m#print(\"Squeezing shape\", x.shape)\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     74\u001b[0m \u001b[39m# print(\"encoded x\", z.shape)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_quant_conv(z)\n",
      "File \u001b[0;32m~/env-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/world/World_Model/models/tokenizer/nets.py:88\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m temb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# timestep embedding\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m# downsampling\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m hs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_in(x)]\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m i_level \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_resolutions):\n\u001b[1;32m     90\u001b[0m     \u001b[39mfor\u001b[39;00m i_block \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_res_blocks):\n",
      "File \u001b[0;32m~/env-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env-py38/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/env-py38/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "train_dataset=loaders['train']\n",
    "trainer = Trans_Trainer(cfg_tokenizer,train_dataset, device)\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
